---
title: Notes FSML II
thanks: Replication files are available on the author's Github account (http://github.com/svmiller/svm-r-markdown-templates). 
author:
  name: Tobías Chavarría
  affiliation: DSTI | DSBD2-001
# date: "`r format(Sys.time(), '%d %B %Y')`" ## Or "Lecture no."
output: 
  html_document:
    keep_md: false
    theme: journal
    highlight: haddock
    # code_folding: show
    toc: yes
    toc_depth: 4
    toc_float: yes
    css: css/preamble.css ## For multi-col environments
  pdf_document:
    latex_engine: xelatex
    keep_tex: false ## Change to true if want keep intermediate .tex file
    toc: true
    toc_depth: 3
    dev: cairo_pdf
    # fig_width: 7 ## Optional: Set default PDF figure width
    # fig_height: 6 ## Optional: Set default PDF figure height
    extra_dependencies: ['booktabs', 'threeparttable', 'float'] # "longtable"
    includes:
      in_header: tex/preamble.tex ## For multi-col environments
    pandoc_args:
        --template=tex/mytemplate.tex ## For affiliation field. See: https://bit.ly/2T191uZ
always_allow_html: true
urlcolor: blue
mainfont: cochineal
## Automatically knit to both formats:
knit: (function(inputFile, encoding) {
 rmarkdown::render(inputFile, encoding = encoding, 
 output_format = 'all') 
 })
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction

Statistics notation:

1. If $X_1, ..., X_n$ are random variables (r.v).
2. $x_1, ..., x_n$ are observations.
3. If we write *i.i.d* means that the r.v are independent and identically distributed.

**First aim:** To propose a model for a random variable.

Generalization to multi-dimensional case:

- Y: response variable.
- $X^{(1)}, ...,X^{(p)}$: explanatory variables.

**Aim**: To find a functional link between $Y$ and the explanatory variables.

To find this functional link , the method to apply depends on the nature of the r.v's.

| Y 	| Model 	|
|:---:	|:---:	|
| Numeric 	| Linear model 	|
| Qualitative (labels) 	| Classification 	|

**Linear model**

A linear model is given by:

$Y_i = \beta_0 + \beta_1X^{1}_i + ... + \beta_pX^{p}_i + \varepsilon_i$

where:

- $\beta_0, ..., \beta_p$ are unknown *fixed* parameters that can be estimated by two methods: 
  - Point estimation
  - Confidence interval
- $\varepsilon$ is the noise and also a random variable.


## Chapter 1: Estimation for **one parameter**

**Previous Knowledge**

- Random Variable: 
 - The notion of distribution.
 - The expectation and variance
 - The distribution function
 - The classical distributions (in particular the Gaussian)
 - The Law of Large numbers and the Central Limit theorem
 
### Introduction

Given $x_1, ..., x_n$ numeric observations, to try to find a correct parametric model, we can use 2 [graphs](https://chartio.com/learn/charts/histogram-complete-guide/):

| Plot type 	| Variable type 	| Density 	|
|:---:	|:---:	|:---:	|
| Bar plot 	| Discrete 	| count/n 	|
| Histogram 	| Continuous 	| count for a bin/ n x (length of the bin) 	|


**Barplot for discrete variables**

```{r create_barplot}
max.temp <- c(22, 27, 26, 24, 23, 26, 28)

barplot(max.temp,
main = "Maximum Temperatures in a Week",
xlab = "Degree Celsius",
ylab = "Day",
names.arg = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"))
```


**Histogram for continuous variables** 


```{r create_histogram}

A <- rnorm(500, 0, 1)

## The default execution of this function doesn't generate a density:
hist(A)


## You need to set freq = FALSE:
hist(A, freq = FALSE)


## You can set the numbers of bins that you want to use:
hist(A, freq = FALSE, breaks = 100)

# But in order to create more breaks, you need to increase the
# numbers of observations:


A <- rnorm(50000, 0, 1)
hist(A, freq = FALSE, breaks = 100)

```


To propose a parametric model:

1. Make a graphical representation of the observations.
2. Guess a theoretical model by looking the previous graphic.

**Examples:** 

Pasarlos a R!!

Example 2: ![](/cloud/project/docs/images/model_1.png)

Example 2: ![](/cloud/project/docs/images/model_2.png)

**Question:** with a representation, we can guess a parametric family of models, denoted by 
$\{P_{\Theta}, \theta \in \Theta \}$.
How to guess a correct value for $\theta$ thanks to the observations?

**Answer:** *Estimation*.

### Point estimation

Let $x_i$ an observation of a r.v $X_i$
we assume that $X_1, ..., X_n$ are *i.i.d* with common distribution $P_{\theta}$.

##### Estimator 

**Definition:** An estimator of $\Theta$ is just a function of $X_1, ... X_n$ that **does not depend onto others unknown parameters.**

**Remark:** An estimator is a random variable! 

##### Estimation 
**Definition:** An estimation is the value of an estimator computed thanks to the observations.

**Example**

Consider $X_1, ..., X_n$ exponential distributed and *i.i.d*, an *estimator* of $\lambda$
is $\hat{\lambda}_n = \dfrac{n}{\sum X_i}$ an *estimation* is
$\hat{\lambda}_n = \dfrac{n}{\sum x_i}$.

| Distribution 	| Parameter 	| Estimator 	| Estimation 	|
|:---:|:---:|:---:|:---:|
|Exponential $\xi(\lambda)$ 	| $\lambda$ 	| $\dfrac{n}{\sum X_i}$|$\dfrac{n}{\sum x_i}$|


##### Bias (for univariate parameter)

**Definition:** Let consider $\hat{\theta}_n$ an estimator of $\theta$.

The bias of $\hat{\theta}_n$ is defined by:

$$b(\hat{\theta}_n) := \mathbb{E}(\hat{\theta}_n) - \theta$$ 

* We say that $\hat{\theta}_n$ is an unbiased estimator if

$$\forall n \in \mathbb{N}^{+} \quad b(\hat{\theta}_n) = 0$$

* We say that $\hat{\theta}_n$ is *asymptotic unbiased* estimator if:

$$b(\hat{\theta}_n) \rightarrow 0 \\ n \rightarrow +\infty$$


**How to construct  estimator?**

- Method of moments
  - less computations
  - based on the Law of large numbers

- Maximum likelihood

#### Method of moments

Let $\theta$ a parameter to estimated, parameter which is associate to  
$X_1, ..., X_n$ *i.i.d* r.v.

Let consider $k \in \mathbb{N}^*$:

- the moment of order $k$ : $\mathbb{E}[X^k]$
- the centered moment of order $k$: $\mathbb{E}[X - \mathbb{E}[X]]^k$

If there exist a value $k$ such that:

- (a) $\mathbb{E}[X^k] =  g(\theta)$

- (b) $\mathbb{E}[[X - \mathbb{E}[X]]^k] =  h(\theta)$

Then the estimator $\hat{\theta}_n$ of $\theta$ is solution of:

- (a) $g(\hat{\theta}_n) = \dfrac{1}{n}\sum X_i^k$

- (b) $h(\hat{\theta}_n) = \dfrac{1}{n}\sum (X_i - \overline{X}_n)^k$ 

where 

$$\overline{X}_n = \dfrac{1}{n}\sum X_i$$ 

is the **empirical mean**.


**Remark: Exponential Distribution**

If Let $X \sim \xi(\lambda)$, then:

- $f_\lambda(x) = \lambda\exp(-\lambda x) \mathbf{1}_{x \geq 0}$
- $\mathbb{E}[X] = \int_{\mathbb{R}} x\cdot f_\lambda(x) dx$
- $V[X] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$

**Transfer formula:**

$$\mathbb{E}[l(X)] =  \int_{\mathbb{R}} l(x)\cdot f_\lambda(x) dx$$

#### Applications

1. Let consider $X_1, ..., X_n$ exponential distributed and *i.i.d* compute an 
estimator of $\lambda$ using the methods of moments.

**Solution:**

Let $X \sim \xi(\lambda)$, so:

$$\mathbb{E}[X] = \dfrac{1}{\lambda}  \quad \text{and} \quad V[X] = \dfrac{1}{\lambda^2}$$
By applying the method of moments ($k=1$) we get:

$\dfrac{1}{\hat{\lambda}_{n,1}} = \dfrac{1}{n}\sum X_i$ 

Thus:

$$\hat{\lambda}_{n,1} = \dfrac{n}{\sum X_i}$$

in the same way but using the variance  ($k=2$), we get:

$\dfrac{1}{\hat{\lambda}_{n,2}^2} = \dfrac{1}{n}\sum (X_i - \overline{X}_n)^2$ 

Thus:

$$\hat{\lambda}_{n,2} = \dfrac{\sqrt{n}}{\sqrt{\sum (X_i - \overline{X}_n)^2}}$$

```{r method_moments}

A = rexp(500, 4)

1/mean(A)

m = c()

for (i in 1:50) {
  A = rexp(500, 4)
  m[i] <- 1/mean(A)
}


mean(m)

boxplot(m)


## With more observations we got less variation (500 -> 5000)

## Law of large numbers

m = c()

for (i in 1:50) {
  A = rexp(50000, 4)
  m[i] <- 1/mean(A)
}


mean(m)

boxplot(m)


```

2. Let consider $X_1, ..., X_n$  *i.i.d*  $\mathcal{U}([0, \sigma])$.

  + Determine an estimator of $\sigma$ using the methods of moments.
  + Let denote $\hat{\sigma}_n = 2\overline{X}_n$. 
  Is $\hat{\sigma}_n$ an unbiased estimator?

**Solution:**

Let $X \sim \mathcal{U}([0, \sigma])$.


\begin{align*}
\mathbb{E}(X) = \int_{\mathbb{R}} x\cdot f_X dx = \int_{\mathbb{R}} x\cdot \dfrac{1}{\sigma}\cdot \mathbf{1}_{[0, \sigma]} dx = \dfrac{1}{\sigma}\int_{0}^{\sigma} x dx = \dfrac{1}{\sigma} \dfrac{x^2}{2} \Big|_0^\sigma = \dfrac{\sigma}{2} 
\end{align*}

By the method of moments, we get that an estimator $\hat{\sigma}_n$ is solution of:

$\dfrac{\hat{\sigma}_n}{2} = \dfrac{1}{n}\sum X_i = \overline{X}_n$

Thus:

$$\hat{\sigma}_n = 2\overline{X}_n$$
since $\hat{\sigma}_n$ it is a function of $X_1, ..., X_n$ it's an estimator.

Let's compute the bias for this estimator, let $n \in \mathbb{N}^*$.

$b(\hat{\sigma}_n ) = \mathbb{E}(\hat{\sigma}_n) - \sigma$

First, compute the expected value of $\hat{\sigma}_n$:


\begin{align*}
\mathbb{E}(\hat{\sigma}_n) &= \mathbb{E}(2\overline{X}_n)\\
&=    \mathbb{E}\bigg[\dfrac{2}{n}\sum X_i\bigg]\\
&=    \dfrac{2}{n}\sum\mathbb{E}[ X_i] \quad\text{(by linearity)}\\
&=    \dfrac{2}{n} \cdot n \cdot\mathbb{E}[X_1] \quad\text{(because are identically distributed)}\\
&=    \sigma
\end{align*}


Therefore $b(\hat{\sigma}_n ) = 0$ and then we conclude that $\hat{\sigma}_n = \overline{X}_n$ is an unbiased estimator for $\sigma$.

3. Let consider $X_1, ..., X_n$  *i.i.d*  $\mathcal{U}([-\sigma, \sigma])$.

Since $\mathbb{E}[X_1] = 0 \quad \text{(not a function of } \sigma)$

we can't use the first moment, then we use the second moment:

$V[X_1] = \dfrac{\sigma^2}{3}$

then we have:

$$\hat{\sigma}_n = \sqrt{\dfrac{3}{n}\sum(X_i - \overline{X}_n)^2}$$

#### The Maximum Likelihood


##### Likelihood

**Definition:** Let $X_1, ..., X_n$ independent random variables, whose distributions are all depending
on the same parameter $\theta$.

Let $x_1, ..., x_n$ observations of those r.v

$$
\mathcal{L}(x_1, ..., x_n, \theta) = \begin{cases} \displaystyle \prod_{i =1}^{n} P_{\theta}(X_i = x_i) \quad \text{(discrete framework)} \\ 
\displaystyle \prod_{i =1}^{n} f_{X_i, \theta}(X_i = x_i) \quad \text{(continuous framework)}
\end{cases}
$$

where $f_{X_i, \theta}$  are the density function of $X_i$.


##### Estimator thanks to the maximum likelihood

**Definition:** $\hat{\theta}_n$, an estimator for $\theta$, due to the maximum likelihood, is solution of:


$$
\mathcal{L}(x_1, ..., x_n, \theta) = \max_{\theta} \mathcal{L}(x_1, ..., x_n, \theta)
$$


#### Applications

1. Let consider $X_1, ..., X_n$ $\xi(\lambda)$ *i.i.d*. Compute the maximum likelihood estimator.

**Solution:**

Let $x_1, ..., x_n \in \mathbb{R}$.


\begin{align*}
\mathcal{L}(x_1, ..., x_n, \theta) &= \displaystyle \prod_{i =1}^{n} \lambda e^{-\lambda x_i}\cdot\mathbf{1}_{x_i \geq 0} \\
&=    \lambda^n e^{-\lambda \sum x_i}\cdot \mathbf{1}_{\min{(x_i) \geq 0}}  
\end{align*}


we need to maximize with respect to $\lambda$, since $\mathbf{1}_{\min(x_i)}$ does not depend on $\lambda$ we can forget it, and consider:

$$h(\lambda) = \lambda^n e^{-\lambda \sum x_i}$$

taking $\log$ in both sides, with get a better expression to deal with.


$$g(\lambda) = \log(h(\lambda)) = \log(\lambda^n e^{-\lambda \sum x_i}) = n\log(\lambda) - \lambda \sum x_i$$
Since

$g'(\lambda) = \dfrac{n}{\lambda} -  \sum x_i$ then $\lambda = \dfrac{n}{ \sum x_i} = \dfrac{1}{\overline{X}_n}$ is critical point.

and $g''(\lambda) = \dfrac{-n}{\lambda^2} <0$ then this critical point correspond to a maximum. 

So, $\lambda = \dfrac{1}{\overline{X}_n}$ is solution of the maximization problem and therefore the *Maximum likelihood estimator* is:

$$\hat{\lambda}_n = \dfrac{n}{ \sum X_i}$$


2. Let consider $X_1, ..., X_n$  *i.i.d* $\mathcal{U}([0, \theta])$
  + Compute the maximum likelihood estimator.
  + Compute the bias of this estimator and transform it into an unbiased one.


**Solution:**

Compute the maximum likelihood estimator:

\begin{align*}
\mathcal{L}(x_1, ..., x_n, \theta) &= \displaystyle \prod_{i =1}^{n} f_{\theta}(x_i) \\
&=    \prod_{i =1}^{n} \dfrac{1}{\theta}\cdot \mathbf{1}_{[0, \theta]}(x_i)  \\
&= \dfrac{1}{\theta^n} \cdot \mathbf{1}_{\min (x_i) \geq 0} \cdot \mathbf{1}_{\max(x_i) \leq \theta}
\end{align*}

Since $\mathbf{1}_{\min (x_i) \geq 0}$ does not depend on $\lambda$ we can forget it, and consider:

$$g(\lambda) = \dfrac{1}{\theta^n} \cdot \mathbf{1}_{\max(x_i) \leq \theta} = \dfrac{1}{\theta^n} \cdot \mathbf{1}_{[\max(x_i), +\infty[} (\theta)$$

we can't compute $g'$ since this function it's not derivable in all points.

But since the function $\dfrac{1}{t^n}$ it's decreasing we can conclude that the maximum estimator likelihood is given by:

$$
\hat{\theta}_n = \max(X_i)
$$

Compute the bias of this estimator and transform it into an unbiased one.

In order to compute the bias of this estimator, we have to compute: $\mathbb{E}(\hat{\theta}_n) = \mathbb{E}(\max(X_i))$.

To do this we need to determine the density function of this new random variable $\max(X_i)$.

**How to compute a density?**

1. First step: Computation of the distribution function.

Let $t \in \mathbb{R}$.


\begin{align*}
F_{\hat{\theta}_n}(t) &= P(\hat{\theta}_n \leq t) \\
&=    P(\max(X_i) \leq t)  \\
\iff \\
&    P(X_1 \leq t, ..., X_n \leq t) \\
&=    \displaystyle \prod_{i =1}^{n} P(X_i \leq t) \quad \text{(by independency of the r.v)} \\
&=    (P(X_1 \leq t))^n \quad \text{(because they are identically distributed.)}
\end{align*}


Since

$$
P(X_1 \leq t) = \int_{-\infty}^{t} \dfrac{1}{\theta} \mathbb{1}_{[0, \theta]}(x)dx = 
\begin{cases}
  0  &  t < 0 \\ \\
  \dfrac{t}{\theta} & t \in [0,\theta] \\\\
  1 & t >1
\end{cases}
$$

We conclude:

$$
F_{\hat{\theta}_n}(t) =
\begin{cases}
  0  &  t < 0 \\\\
  \dfrac{t^n}{\theta^n} & t \in [0,\theta] \\\\
  1 & t >1
\end{cases}
$$

2. Second step: Computation of the density function.

Using the relation between the distribution and density function we have:

$$
f_{\hat{\theta}_n}(t) = F'_{\hat{\theta}_n}(t) = n\cdot\dfrac{t^{n-1}}{\theta^n}\cdot\mathbf{1}_{[0, \theta]}(t)
$$

3. Third step: Computation of $\mathbb{E}(\hat{\theta}_n)$

$$ \mathbb{E}[\hat{\theta}_n] = \int_{\mathbb{R}} x \cdot f_{\hat{\theta}_n}(x) dx = \dfrac{n}{n+1} \theta$$

Now we have:

$$ b(\hat{\theta}_n) = \mathbb{E}[\hat{\theta}_n] - \theta = -\dfrac{\theta}{n+1} \neq 0$$

Therefore $\hat{\theta}_n$ is not an unbiased estimator for $\theta$. But since $\displaystyle \lim_{n\to\infty} b(\hat{\theta}_n) = 0$ it is an asymptotically unbiased estimator for $\theta$.


**How to obtain an unbiased estimator?**

- First attempt: 

We have $\mathbb{E}[\hat{\theta}_n] - \theta = -\dfrac{\theta}{n+1}$, then:

\begin{align*}
\mathbb{E}[\hat{\theta}_n] - \theta + \dfrac{\theta}{n+1} &= 0 \\
 \iff & \\
\mathbb{E}\bigg[\hat{\theta}_n + \dfrac{\theta}{n+1}\bigg] - \theta  &= 0 \\
\end{align*}

It is ok to consider $\tilde{\theta}_n = \hat{\theta}_n + \dfrac{\theta}{n+1}$ as unbiased estimator of $\theta$?

The answer is **no**, $\tilde{\theta}_n$ depends on an unkonown parameter ($\theta$ in this case), so by definition it is not an estimator!


- Second attempt: 

We have:


\begin{align*}
\mathbb{E}[\hat{\theta}_n] - \theta =- \dfrac{\theta}{n+1} &= 0 \\
 \iff & \\
\mathbb{E}[\hat{\theta}_n] = \theta - \dfrac{\theta}{n+1}  \\
 \iff & \\
\mathbb{E}[\hat{\theta}_n] = \dfrac{n}{n+1}\theta \\
 \iff & \\
\mathbb{E}\bigg[\dfrac{n+1}{n}\hat{\theta}_n\bigg] = \theta \\
 \iff & \\
\mathbb{E}\bigg[\dfrac{n+1}{n}\hat{\theta}_n\bigg] - \theta = 0\\
\end{align*}

Thus, $\dfrac{n+1}{n}\hat{\theta}_n$ is an unbiased estimator for $\theta$.

```{r estimator_unif}

## Comparing the estimators for the uniform distribution.

n= 100

U = runif(n, 0, 4)

theta = max(U)


for (i in 1:50) {
  U = runif(n, 0, 4)
  theta = c(theta, max(U))
}


mean(theta)

boxplot(theta)


## Adjust to make the estimator unbiased

thetab = (n+1)/n*theta

mean(thetab)

boxplot(thetab)


## With more observations

n= 100

U = runif(n, 0, 4)

theta = max(U)


for (i in 1:5000) {
  U = runif(n, 0, 4)
  theta = c(theta, max(U))
}


mean(theta)

boxplot(theta)

```

#### Property

Let $X_1, ..., X_n$ *i.i.d* random variables.

- Let $\mu = \mathbb{E}[X_1]$ (unknown)
- Let $\sigma^2 = V[X_1]$ (unknown)

A classical estimator for:

- $\mu$ is 
$$\hat{\mu}_n = \overline{X}_n$$

- $\sigma^2$ is 

$$\hat{\sigma}_n^2 = \dfrac{1}{n} \sum(X_i - \overline{X}_n)^2$$


**Exercise**

Show that:

1. $\hat{\mu}$ is unbiased.
2. $\hat{\sigma}^2_n$ is biased and that $\dfrac{n}{n-1}\hat{\sigma}^2_n$ is unbiased.



## Day 2 (In progress)

- Solution exercise
- Theorem

### Quality of an estimator

**def:** Let $\theta$ an unknown parameter, let $\hat{\theta}_n$ an estimator of $\theta$,
mean quadratic error is given by:


**Property:** 

$$MQE = V[\hat{\theta}_n] + ((b(\hat{\theta}_n)))^2$$

**Proof:**

\begin{equation}
MQE(\hat{\theta}_n) = ...
\end{equation}

##theory##


### Practical class

1. Load the data in R software.
2. Propose a model for the variables associated to this file.
  + Make a visualization of this.


```{r read_data}

data <- as.matrix(read.table("/cloud/project/docs/data/data1.txt"))

## Some comments about the data and data types.

```

```{r graphic_data}

hist(data, freq = FALSE)


## Be careful about getting conclusions

data1_min = min(data)
data1_max = max(data)

hist(data, freq = FALSE, breaks = seq(data1_min, data1_max, length = 10))

```

We guess a Gaussian distribution.

How to estimate the parameters of the distribution?

1. Compute the empirical mean and statistical variance.
2. Plot the theoretical density that we guess.
3. Test the goodness of fitness.


Compute the empirical mean and statistical variance.

```{r compute_parameters}

mhu <- mean(data)

# When you compute the variance be careful and read the documentation
# in this case we have:
# "The denominator n - 1 is used which gives an unbiased estimator of the (co)variance 
# for i.i.d. observations"

sigma2 <- var(data)

## You also can do it manually:

sigma2 <- 1/(nrow(data)-1)*sum((data - mean(data))^2)

```

Plot the theoretical density that we guess.

```{r get_hit_info}

## This generates a list, with all the hist information.

H <- hist(data, freq = FALSE, plot = FALSE) ## ignore warning message
H

```
```{r make_comparison}

## Histogram info to use.
limits <- H$breaks
lmin <- limits[1]
lmax <- limits[length(limits)]

## Create our plot

x <- seq(lmin, lmax, by = 0.01)
y <- dnorm(x, mhu, sqrt(sigma2))


y_max = max(y, H$density)

## Make the comparison 

hist(data, freq = FALSE, xlim = c(lmin, lmax), ylim = c(0, y_max*1.01))
par(new = TRUE)
plot(x, y, type = 'l', col = 'red'
     , xlim = c(lmin, lmax)
     , ylim = c(0, y_max*1.01)
     , xlab = ""
     , ylab = "")
```

Test the goodness of fitness.

```{r test_results}

ks.test(x, 'pnorm', mhu, sqrt(sigma2))

```

### Data set 2

```{r}

## Loading the data
A <- as.matrix(read.table("/cloud/project/docs/data/data2.txt"))


## Make visualizations

hist(A, freq = FALSE)
## The right box exist because r creates one class with just one element.

## There is a formula that generates the right amount of classes, given by:

## K aproxx 1 + 3.22*log(n, 10) ## (in practice take the floor)



## Use right parameters:

n = length(A)
K = floor(1 + 3.22*log(n, 10))
A_min = min(A)
A_max = max(A)

A_mp = (A_max - A_min)/K

epsilon = (A_max - A_min)/10^9

limits = seq(A_min, A_max, by = A_mp)


hist(A, freq = FALSE, breaks = limits)

## We guess is an uniform distribution

## Make start in 0

A_zero = A - min(A)

A_min = min(A_zero)
A_max = max(A_zero)
hist(A_zero, freq = FALSE, breaks = seq(A_min, A_max, length = 10))

## Estimate the parameter

n <- length(A_zero)

theta <- ((n+1)/n)*max(A_zero)

## Create our theoretical density plot


H <- hist(A_min, freq = FALSE, plot = FALSE) ## ignore warning message
## Histogram info to use.
limits <- H$breaks
lmin <- limits[1]
lmax <- limits[length(limits)]

## Create our plot

x <- seq(lmin, lmax, by = 0.01)
y <- dunif(x, min = lmin, max = theta )

y_max = max(y, H$density)

## Make the comparison 

hist(A_min, freq = FALSE, xlim = c(lmin, lmax), ylim = c(0, y_max*1.01))
par(new = TRUE)
plot(x, y, type = 'l', col = 'red'
     , xlim = c(lmin, lmax)
     , ylim = c(0, y_max*1.01)
     , xlab = ""
     , ylab = "")


```


### Discrete case


data4.txt

```{r}



## Loading the data
A <- as.matrix(read.table("/cloud/project/docs/data/data4.txt"))

barplot(table(A)/length(A))

## A binomial distribution can be approach by a Gaussian distribution.

## Methods of moments
## Don't forget to correct the estimation of np to be an integer.
## and then correct the ph value 




m = mean(A)
s2 = var(A)*(499/500)

ph = 1-s2/m
nh = m/ph

nh

nh = 10

ph = m/nh

ph

```

data5.txt

```{r}


## Loading the data
A <- as.matrix(read.table("/cloud/project/docs/data/data5.txt"))

barplot(table(A)/length(A))

## A binomial distribution can be approach by a Gaussian distribution.

## Methods of moments
## Don't forget to correct the estimation of np to be an integer.
## and then correct the ph value 


m = mean(A)
s2 = var(A)*(499/500)

ph = 1-s2/m
nh = m/ph

nh

nh = 7

ph = m/nh

ph

## When p is small is very hard to get correct estimations of n when you 
## don't have enough number of observations
```


## Day 3 (In progress)


### Confidence interval

```{r}
m1 = c()

for (i in 1:50) {
  A = rnorm(500, 2, 1)
  
  m1 = c(m1, mean(A))
}

boxplot(A)

## Increase the number of observations

m2 = c()

for (i in 1:50) {
  B = rnorm(2000, 2, 1)
  
  m2 = c(m2, mean(B))
}

boxplot(B)


## Change the variance

m3 = c()

for (i in 1:50) {
  C = rnorm(2000, 2, 10)
  
  m3 = c(m3, mean(C))
}

boxplot(C)

## Create a data frame with all the observations

a = c(m1, m2, m3)
r = rep(c("A", "B", "C"), each = 50)

S = data.frame(x = a, l = r)
```
In this plot we can see that the approximation depends on the number of observations and the variance of the sample.

```{r}
boxplot(S$x ~ S$l)
```




### Chi squared distribution

```{r}
x = seq(0.1, 10, by =0.01)

y1 = dchisq(x, 1)
y2 = dchisq(x, 2)
y3 = dchisq(x, 3)
y4 = dchisq(x, 4)
y6 = dchisq(x, 6)
y9 = dchisq(x, 9)

y_max = max(y1, y2, y3, y4, y6, y9)

plot(x, y1, type = 'l', col = 'yellow', xlim = c(0, 10), ylim = c(0, 0.5))
par(new = TRUE)
plot(x, y2, type = 'l', col = 'green', xlim = c(0, 10), ylim = c(0, 0.5))
par(new = TRUE)
plot(x, y3, type = 'l', col = 'black', xlim = c(0, 10), ylim = c(0, 0.5))
par(new = TRUE)
plot(x, y4, type = 'l', col = 'blue', xlim = c(0, 10), ylim = c(0, 0.5))
par(new = TRUE)
plot(x, y6, type = 'l', col = 'pink', xlim = c(0, 10), ylim = c(0, 0.5))
par(new = TRUE)
plot(x, y9, type = 'l', col = 'red', xlim = c(0, 10), ylim = c(0, 0.5))
```



### t-student distribution

```{r}
x = seq(-10, 10, by =0.01)

y1 = dt(x, 1)
y2 = dt(x, 2)
y3 = dt(x, 3)
y4 = dt(x, 4)
y6 = dt(x, 6)
y20 = dt(x, 20)

y_norm = dnorm(x)

y_max = max(y1, y2, y3, y4, y6, y9)

plot(x, y1, type = 'l', col = 'yellow', xlim = c(-10, 10), ylim = c(0, y_max))
par(new = TRUE)
plot(x, y2, type = 'l', col = 'green', xlim = c(-10, 10), ylim = c(0, y_max))
par(new = TRUE)
plot(x, y3, type = 'l', col = 'black', xlim = c(-10, 10), ylim = c(0, y_max))
par(new = TRUE)
plot(x, y4, type = 'l', col = 'blue', xlim = c(-10, 10), ylim = c(0, y_max))
par(new = TRUE)
plot(x, y6, type = 'l', col = 'pink', xlim = c(-10, 10), ylim = c(0, y_max))
par(new = TRUE)
plot(x, y20, type = 'l', col = 'red', xlim = c(-10, 10), ylim = c(0, y_max))
par(new = TRUE)
plot(x, y_norm, type = 'l', col = 'black', xlim = c(-10, 10), ylim = c(0, y_max))


## A t-student distribution with a big enough k approach a Gaussian distribution

```

