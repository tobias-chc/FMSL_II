---
title: Notes FSML II
thanks: Replication files are available on the author's Github account (http://github.com/svmiller/svm-r-markdown-templates). 
author:
  name: Tobías Chavarría
  affiliation: DSTI | DSBD2-001
# date: "`r format(Sys.time(), '%d %B %Y')`" ## Or "Lecture no."
output: 
  html_document:
    keep_md: false
    theme: journal
    highlight: haddock
    # code_folding: show
    toc: yes
    toc_depth: 4
    toc_float: yes
    css: css/preamble.css ## For multi-col environments
  pdf_document:
    latex_engine: xelatex
    keep_tex: false ## Change to true if want keep intermediate .tex file
    toc: true
    toc_depth: 3
    dev: cairo_pdf
    # fig_width: 7 ## Optional: Set default PDF figure width
    # fig_height: 6 ## Optional: Set default PDF figure height
    extra_dependencies: ['booktabs', 'threeparttable', 'float'] # "longtable"
    includes:
      in_header: tex/preamble.tex ## For multi-col environments
    pandoc_args:
        --template=tex/mytemplate.tex ## For affiliation field. See: https://bit.ly/2T191uZ
always_allow_html: true
urlcolor: blue
mainfont: cochineal
## Automatically knit to both formats:
knit: (function(inputFile, encoding) {
 rmarkdown::render(inputFile, encoding = encoding, 
 output_format = 'all') 
 })
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction

Statistics notation:

1. If $X_1, ..., X_n$ are random variables (r.v).
2. $x_1, ..., x_n$ are observations.
3. If we write *i.i.d* means that the r.v are independent and identically distributed.

**First aim:** To propose a model for a random variable.

Generalization to multi-dimensional case:

- Y: response variable.
- $X^{(1)}, ...,X^{(p)}$: explanatory variables.

**Aim**: To find a functional link between $Y$ and the explanatory variables.

To find this functional link , the method to apply depends on the nature of the r.v's.

| Y 	| Model 	|
|---	|---	|
| Numeric 	| Linear model 	|
| Qualitative (labels) 	| Classification 	|

**Linear model**

A linear model is given by:

$Y_i = \beta_0 + \beta_1X^{1}_i + ... + \beta_pX^{p}_i + \varepsilon_i$

where:

- $\beta_0, ..., \beta_p$ are unknown *fixed* parameters that can be estimated by two methods: 
  - Point estimation
  - Confidence interval
- $\varepsilon$ is the noise and also a random variable.


## Chapter 1: Estimation for **one parameter**

**Previous Knowledge**

- Random Variable: 
 - The notion of distribution.
 - The expectation and variance
 - The distribution function
 - The classical distributions (in particular the Gaussian)
 - The Law of Large numbers and the Central Limit theorem
 
### Introduction

Given $x_1, ..., x_n$ numeric observations, to try to find a correct parametric model, we can use 2 [graphs](https://chartio.com/learn/charts/histogram-complete-guide/):

- Barplot for discrete variables.

density = count/n

```{r create_barplot}
max.temp <- c(22, 27, 26, 24, 23, 26, 28)

barplot(max.temp,
main = "Maximum Temperatures in a Week",
xlab = "Degree Celsius",
ylab = "Day",
names.arg = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"))
```


- Histogram for continuous variables 

density = count for a bin/ n x length of the bin

```{r create_histogram}

A <- rnorm(500, 0, 1)

hist(A)

hist(A, freq = FALSE)

hist(A, freq = FALSE, breaks = 20)
hist(A, freq = FALSE, breaks = 100)

# In order to create more breaks, you need to increase the
# numbers of observations


A <- rnorm(50000, 0, 1)
hist(A, freq = FALSE, breaks = 100)

```

To propose a parametric model:

1. We make a graphical representation of the observations.
2. We guess a theoretical model by looking the previous graphic.


**Question:** with a representation, we can guess a parametric family of models, denoted by 
$\{P_{\Theta}, \theta \in \Theta \}$.
How to guess a correct value for $\theta$ thanks to the observations?

**Answer:** *Estimation*.


### Point estimation

$x_i$ an observation of a r.v $X_i$
we assume that $x_1, ..., x_n$ are *i.i.d* with common distribution $P_{\Theta}$.

**Def**: Estimator
An estimator of $\Theta$ is just a function of $X_1, ... X_n$ that **does not depend onto others unknown parameters.**

**Rk**: An estimator is a random variable!

**Def:** Estimation
An estimation is the value of an estimator computed thanks to the observations.

**Example**

Consider $X_1, ..., X_n$ exponential distributed and *i.i.d*, an *estimator* of $\lambda$
is $\hat{\lambda}_n = \dfrac{n}{\sum X_i}$ an *estimation* is
$\hat{\lambda}_n = \dfrac{n}{\sum x_i}$.

**Def:**  Bias (for univariate parameter)

Let consider $\hat{\Theta}_n$ an estimator of $\Theta$.

The bias of $\hat{\Theta}_n$ is defined by:

$b(\hat{\Theta}_n) := \mathbb{E}(\hat{\Theta}_n) - \Theta$ 

We say that $\hat{\Theta}_n$ is an unbiased estimator if
$\forall n \in \mathbb{N}^{+} \quad b(\hat{\Theta}_n) = 0$

We say that $\hat{\Theta}_n$ is asymptotic unbiased estimator if:

$b(\hat{\Theta}_n) \rightarrow 0$ as $n \rightarrow \infty +$

**How to construct  estimator?**

- Method of moments
  - less computations
  - based on the Law of large numbers

- Maximum likelihood

#### Method of moments

Let $\Theta$ a parameter to estimated, parameter which is associate to  
$X_1, ..., X_n$ *i.i.d* r.v.

Let consider $k \in \mathbb{N}^*$:

- the moment of order $k$ : $\mathbb{E}[x^k]$
- the centered moment of order $k$: $\mathbb{E}[x - \mathbb{E}[x]]^k$

If there exist a value $k$ such that:

- (a) $\mathbb{E}[x^k] =  g(\Theta)$

- (b) $\mathbb{E}[x - \mathbb{E}[x]]^k =  h(\Theta)$

*Applications*

Let consider $X_1, ..., X_n$ exponential distributed and *i.i.d*

Solution:

...

```{r method_moments}

A = rexp(500, 4)

1/mean(A)

m = c()

for (i in 1:50) {
  A = rexp(500, 4)
  m[i] <- 1/mean(A)
}


mean(m)

boxplot(m)


## With more observations we got less variation (500 -> 5000)

## Law of large numbers

m = c()

for (i in 1:50) {
  A = rexp(50000, 4)
  m[i] <- 1/mean(A)
}


mean(m)

boxplot(m)


```



#### The Maximum Likelihood


**Def:** likelihood

Let $X_1, ..., X_n$ independent random variables, whose distributions are all depending
on the same parameter $\Theta$.

Let $x_1, ..., x_n$ observations of those r.v

$$
\mathcal{L}(x_1, ..., x_n, \Theta) = \begin{cases} \Pi{} \\ \Pi \end{cases}
$$

**Def:** Estimator thanks to the maximum likelihood.

$\hat{\Theta}_n$, an estimator for $\Theta$, due to the maximum likelihood, is solution of:


$$
\mathcal{L}(x_1, ..., x_n, \Theta) = \max_{\theta} \mathcal{L}(x_1, ..., x_n, \Theta)
$$

*Applications*

Let consider $X_1, ..., X_n$ $\xi(\lambda)$ *i.i.d*.

Compute the maximum likelihood estimator.

Solution:

...


```{r}

n= 100

U = runif(n, 0, 4)

theta = max(U)


for (i in 1:50) {
  U = runif(n, 0, 4)
  theta = c(theta, max(U))
}


mean(theta)

boxplot(theta)


## Adjust to make the estimator unbiased

thetab = (n+1)/n*theta

mean(thetab)

boxplot(thetab)


## With more observations

n= 100

U = runif(n, 0, 4)

theta = max(U)


for (i in 1:5000) {
  U = runif(n, 0, 4)
  theta = c(theta, max(U))
}


mean(theta)

boxplot(theta)


```


**Property**

Let $X_1, ..., X_n$ *i.i.d* r.v.
Let $\mu = \mathbb{E}[X_1]$ (unknown)
Let $\sigma^2 = V(X_1)$ (unknown)

A classical estimator is:

- $\mu$ is $\hat{\mu}_n = \overline{X}_n = \frac{1}{n} \sum X_i$
- $\sigma^2$ is $\hat{\sigma}^2_n = \frac{1}{n} \sum (X_i - \overline{X}_n)^2$


**Exercise**

Show that:

1. $\hat{\mu}$ is unbiased.
2. $\hat{\sigma}^2_n$ is biased and that $\dfrac{n}{n-1}\hat{\sigma}^2_n$ is unbiased.



